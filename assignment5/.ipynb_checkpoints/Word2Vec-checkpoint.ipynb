{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 5.1 - Word2Vec\n",
    "\n",
    "В этом задании мы натренируем свои word vectors на очень небольшом датасете.\n",
    "Мы будем использовать самую простую версию word2vec, без negative sampling и других оптимизаций.\n",
    "\n",
    "Перед запуском нужно запустить скрипт `download_data.sh` чтобы скачать данные.\n",
    "\n",
    "Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We'll use Principal Component Analysis (PCA) to visualize word vectors,\n",
    "# so make sure you install dependencies from requirements.txt!\n",
    "from sklearn.decomposition import PCA \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tokens: 19538\n",
      "tackling ['inexperienced', 'teeth']\n",
      "apply ['cynics', 'need']\n",
      "yale ['john', 'pogue', 'grad', 'previously']\n",
      "not ['narratively', 'cohesive']\n",
      "remembrance ['first', 'important']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class StanfordTreeBank:\n",
    "    '''\n",
    "    Wrapper for accessing Stanford Tree Bank Dataset\n",
    "    https://nlp.stanford.edu/sentiment/treebank.html\n",
    "    \n",
    "    Parses dataset, gives each token and index and provides lookups\n",
    "    from string token to index and back\n",
    "    \n",
    "    Allows to generate random context with sampling strategy described in\n",
    "    word2vec paper:\n",
    "    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.index_by_token = {}\n",
    "        self.token_by_index = []\n",
    "\n",
    "        self.sentences = []\n",
    "\n",
    "        self.token_freq = {}\n",
    "        \n",
    "        self.token_reject_by_index = None\n",
    "\n",
    "    def load_dataset(self, folder):\n",
    "        filename = os.path.join(folder, \"datasetSentences.txt\")\n",
    "\n",
    "        with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "            l = f.readline() # skip the first line\n",
    "            \n",
    "            for l in f:\n",
    "                splitted_line = l.strip().split()\n",
    "                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n",
    "                    \n",
    "                self.sentences.append(words)\n",
    "                for word in words:\n",
    "                    if word in self.token_freq:\n",
    "                        self.token_freq[word] +=1 \n",
    "                    else:\n",
    "                        index = len(self.token_by_index)\n",
    "                        self.token_freq[word] = 1\n",
    "                        self.index_by_token[word] = index\n",
    "                        self.token_by_index.append(word)\n",
    "                        \n",
    "        self.compute_token_prob()\n",
    "                        \n",
    "    def compute_token_prob(self):\n",
    "        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n",
    "        words_freq = words_count / np.sum(words_count)\n",
    "        \n",
    "        # Following sampling strategy from word2vec paper:\n",
    "        # https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n",
    "    \n",
    "    def check_reject(self, word):\n",
    "        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n",
    "        \n",
    "    def get_random_context(self, context_length=5):\n",
    "        \"\"\"\n",
    "        Returns tuple of center word and list of context words\n",
    "        \"\"\"\n",
    "        sentence_sampled = []\n",
    "        while len(sentence_sampled) <= 2:\n",
    "            sentence_index = np.random.randint(len(self.sentences)) \n",
    "            sentence = self.sentences[sentence_index]\n",
    "            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n",
    "    \n",
    "        center_word_index = np.random.randint(len(sentence_sampled))\n",
    "        \n",
    "        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n",
    "        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n",
    "        \n",
    "        return sentence_sampled[center_word_index], words_before+words_after\n",
    "    \n",
    "    def num_tokens(self):\n",
    "        return len(self.token_by_index)\n",
    "        \n",
    "data = StanfordTreeBank()\n",
    "data.load_dataset(\"./stanfordSentimentTreebank/\")\n",
    "\n",
    "print(\"Num tokens:\", data.num_tokens())\n",
    "for i in range(5):\n",
    "    center_word, other_words = data.get_random_context(5)\n",
    "    print(center_word, other_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Имплеменируем PyTorch-style Dataset для Word2Vec\n",
    "\n",
    "Этот Dataset должен сгенерировать много случайных контекстов и превратить их в сэмплы для тренировки.\n",
    "\n",
    "Напоминаем, что word2vec модель получает на вход One-hot вектор слова и тренирует простую сеть для предсказания на его основе соседних слов.\n",
    "Из набора слово-контекст создается N сэмплов (где N - количество слов в контексте):\n",
    "\n",
    "Например:\n",
    "\n",
    "Слово: `orders` и контекст: `['love', 'nicest', 'to', '50-year']` создадут 4 сэмпла:\n",
    "- input: `orders`, target: `love`\n",
    "- input: `orders`, target: `nicest`\n",
    "- input: `orders`, target: `to`\n",
    "- input: `orders`, target: `50-year`\n",
    "\n",
    "Все слова на входе и на выходе закодированы через one-hot encoding, с размером вектора равным количеству токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample - input: tensor([0., 0., 0.,  ..., 0., 0., 0.]), target: 18\n"
     ]
    }
   ],
   "source": [
    "class Word2VecPlain(Dataset):\n",
    "    '''\n",
    "    PyTorch Dataset for plain Word2Vec.\n",
    "    Accepts StanfordTreebank as data and is able to generate dataset based on\n",
    "    a number of random contexts\n",
    "    '''\n",
    "    def __init__(self, data, num_contexts=30000):\n",
    "        '''\n",
    "        Initializes Word2VecPlain, but doesn't generate the samples yet\n",
    "        (for that, use generate_dataset)\n",
    "        Arguments:\n",
    "        data - StanfordTreebank instace\n",
    "        num_contexts - number of random contexts to use when generating a dataset\n",
    "        '''\n",
    "        # TODO: Implement what you need for other methods!\n",
    "        \n",
    "        self.data = data\n",
    "        self.num_contexts  = num_contexts\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        '''\n",
    "        Generates dataset samples from random contexts\n",
    "        Note: there will be more samples than contexts because every context\n",
    "        can generate more than one sample\n",
    "        '''\n",
    "        # TODO: Implement generating the dataset\n",
    "        # You should sample num_contexts contexts from the data and turn them into samples\n",
    "        # Note you will have several samples from one context\n",
    "        self.dataset = []\n",
    "        \n",
    "        for sample in range(self.num_contexts):\n",
    "            center_word, other_words = data.get_random_context(context_length=5)\n",
    "            \n",
    "            for context_word in other_words:\n",
    "                \n",
    "               \n",
    "                self.dataset.append([center_word, context_word])\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Returns total number of samples\n",
    "        '''\n",
    "        # TODO: Return the number of samples\n",
    "        return self.num_contexts \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Returns i-th sample\n",
    "        \n",
    "        Return values:\n",
    "        input_vector - torch.Tensor with one-hot representation of the input vector\n",
    "        output_index - index of the target word (not torch.Tensor!)\n",
    "        '''\n",
    "        # TODO: Generate tuple of 2 return arguments for i-th sample \n",
    "        \n",
    "        center_word_one_hot_repr = np.zeros(self.data.num_tokens())\n",
    "        center_word_one_hot_repr[self.data.index_by_token[self.dataset[index][0]]] = 1\n",
    "\n",
    "        \n",
    "        center_word_one_hot_repr_tensor = torch.tensor(center_word_one_hot_repr).type(torch.FloatTensor)\n",
    "        target_index = self.data.index_by_token[self.dataset[index][1]]\n",
    "        \n",
    "        return center_word_one_hot_repr_tensor , target_index\n",
    "\n",
    "dataset = Word2VecPlain(data, 10)\n",
    "dataset.generate_dataset()\n",
    "input_vector, target = dataset[3]\n",
    "print(\"Sample - input: %s, target: %s\" % (input_vector, int(target))) # target should be able to convert to int\n",
    "assert isinstance(input_vector, torch.Tensor)\n",
    "assert torch.sum(input_vector) == 1.0\n",
    "assert input_vector.shape[0] == data.num_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создаем модель и тренируем ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=19538, out_features=10, bias=False)\n",
       "  (1): Linear(in_features=10, out_features=19538, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the usual PyTorch structures\n",
    "dataset = Word2VecPlain(data, 30000)\n",
    "dataset.generate_dataset()\n",
    "\n",
    "wordvec_dim = 10\n",
    "\n",
    "# We can use a standard sequential model for this\n",
    "nn_model = nn.Sequential(\n",
    "            nn.Linear(data.num_tokens(), wordvec_dim, bias=False),\n",
    "            nn.Linear(wordvec_dim, data.num_tokens(), bias=False),\n",
    "    )\n",
    "\n",
    "nn_model.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_vectors(nn_model):\n",
    "    '''\n",
    "    Extracts word vectors from the model\n",
    "    \n",
    "    Returns:\n",
    "    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n",
    "    '''\n",
    "    # TODO: Implement extracting word vectors from param weights\n",
    "    # return tuple of input vectors and output vectos \n",
    "    # Hint: you can access weights as Tensors through nn.Linear class attributes\n",
    " \n",
    "    return nn_model[0].weight.detach().numpy().T, nn_model[1].weight.detach().numpy()\n",
    "    \n",
    "extract_word_vectors(nn_model)\n",
    "untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n",
    "\n",
    "assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataset, train_loader, optimizer, scheduler, num_epochs):\n",
    "    '''\n",
    "    Trains plain word2vec using cross-entropy loss and regenerating dataset every epoch\n",
    "    \n",
    "    Returns:\n",
    "    loss_history, train_history\n",
    "    '''\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        dataset.generate_dataset() # Regenerate dataset every epoch\n",
    "        \n",
    "        # TODO Implement training for this model\n",
    "        # Note we don't have any validation set here because our purpose is the word vectors,\n",
    "        # not the predictive performance of the model\n",
    "        #\n",
    "        # And don't forget to step the learing rate scheduler!  \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x,y) in enumerate(train_loader):\n",
    "            \n",
    "            prediction = model(x)\n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "        \n",
    "        scheduler.step()\n",
    "                                \n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        \n",
    "        print(\"Epoch %i, Average loss: %f, Train accuracy: %f\" % (epoch, ave_loss, train_accuracy))\n",
    "        \n",
    "    return loss_history, train_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну и наконец тренировка!\n",
    "\n",
    "Добейтесь значения ошибки меньше **8.0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.1, 0.01, 64, 6, 64],\n",
       " [0.010000000000000002, 0.01, 8, 2, 32],\n",
       " [0.010000000000000002, 0.01, 32, 5, 16],\n",
       " [0.1, 0.01, 16, 4, 16],\n",
       " [0.0010000000000000002, 0.01, 8, 3, 64],\n",
       " [0.0010000000000000002, 0.0001, 8, 2, 64],\n",
       " [0.1, 0.0001, 128, 5, 8],\n",
       " [0.010000000000000002, 0.01, 64, 5, 64],\n",
       " [0.1, 0.01, 16, 2, 32],\n",
       " [0.1, 0.0001, 8, 2, 64],\n",
       " [0.010000000000000002, 0.0001, 64, 2, 32],\n",
       " [0.1, 0.0001, 16, 2, 16],\n",
       " [0.010000000000000002, 0.01, 16, 4, 16],\n",
       " [0.1, 0.01, 32, 3, 8],\n",
       " [0.0010000000000000002, 0.01, 8, 6, 16],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 8, 4, 32],\n",
       " [0.0010000000000000002, 0.0001, 128, 5, 64],\n",
       " [0.1, 1.0000000000000002e-06, 32, 6, 16],\n",
       " [0.010000000000000002, 0.01, 64, 3, 8],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 64, 4, 64],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 16, 2, 16],\n",
       " [0.0010000000000000002, 0.0001, 8, 5, 8],\n",
       " [0.010000000000000002, 0.01, 16, 4, 64],\n",
       " [0.1, 1.0000000000000002e-06, 64, 3, 8],\n",
       " [0.1, 1.0000000000000002e-06, 64, 5, 8],\n",
       " [0.1, 0.01, 64, 4, 16],\n",
       " [0.0010000000000000002, 0.0001, 8, 3, 64],\n",
       " [0.010000000000000002, 0.01, 8, 2, 8],\n",
       " [0.0010000000000000002, 0.0001, 32, 2, 32],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 16, 2, 16],\n",
       " [0.1, 0.0001, 32, 6, 64],\n",
       " [0.010000000000000002, 0.0001, 8, 5, 64],\n",
       " [0.1, 1.0000000000000002e-06, 32, 2, 16],\n",
       " [0.1, 1.0000000000000002e-06, 8, 6, 32],\n",
       " [0.010000000000000002, 0.01, 64, 2, 16],\n",
       " [0.0010000000000000002, 0.01, 32, 5, 64],\n",
       " [0.1, 0.0001, 16, 2, 32],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 128, 6, 16],\n",
       " [0.1, 0.01, 8, 3, 64],\n",
       " [0.0010000000000000002, 0.01, 16, 2, 8],\n",
       " [0.1, 1.0000000000000002e-06, 8, 5, 8],\n",
       " [0.0010000000000000002, 0.01, 128, 2, 8],\n",
       " [0.1, 0.0001, 128, 4, 64],\n",
       " [0.1, 1.0000000000000002e-06, 64, 2, 32],\n",
       " [0.0010000000000000002, 0.01, 128, 4, 32],\n",
       " [0.0010000000000000002, 0.0001, 16, 2, 16],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 16, 6, 32],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 128, 5, 8],\n",
       " [0.010000000000000002, 0.01, 32, 2, 64],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 64, 6, 32],\n",
       " [0.0010000000000000002, 0.01, 128, 2, 64],\n",
       " [0.010000000000000002, 0.0001, 128, 4, 16],\n",
       " [0.010000000000000002, 0.01, 16, 2, 16],\n",
       " [0.1, 0.0001, 8, 4, 16],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 8, 5, 8],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 32, 4, 16],\n",
       " [0.0010000000000000002, 0.0001, 8, 2, 64],\n",
       " [0.1, 0.0001, 8, 6, 16],\n",
       " [0.1, 1.0000000000000002e-06, 64, 5, 8],\n",
       " [0.1, 0.01, 128, 5, 16],\n",
       " [0.1, 1.0000000000000002e-06, 64, 4, 8],\n",
       " [0.010000000000000002, 0.01, 16, 2, 32],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 16, 4, 64],\n",
       " [0.010000000000000002, 0.0001, 8, 3, 8],\n",
       " [0.1, 0.01, 128, 3, 8],\n",
       " [0.1, 0.01, 64, 2, 32],\n",
       " [0.1, 0.01, 128, 4, 16],\n",
       " [0.1, 0.0001, 64, 5, 64],\n",
       " [0.1, 0.0001, 32, 3, 64],\n",
       " [0.0010000000000000002, 0.01, 32, 5, 32],\n",
       " [0.1, 0.0001, 16, 5, 64],\n",
       " [0.0010000000000000002, 0.0001, 64, 4, 16],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 8, 6, 64],\n",
       " [0.0010000000000000002, 0.01, 64, 6, 8],\n",
       " [0.010000000000000002, 0.01, 64, 5, 8],\n",
       " [0.010000000000000002, 0.0001, 64, 6, 16],\n",
       " [0.1, 0.01, 128, 6, 32],\n",
       " [0.010000000000000002, 0.01, 16, 3, 16],\n",
       " [0.1, 0.0001, 128, 2, 64],\n",
       " [0.1, 0.0001, 32, 2, 32],\n",
       " [0.1, 0.01, 16, 6, 32],\n",
       " [0.1, 0.0001, 128, 4, 32],\n",
       " [0.010000000000000002, 0.01, 128, 4, 64],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 8, 6, 32],\n",
       " [0.0010000000000000002, 0.01, 64, 4, 8],\n",
       " [0.010000000000000002, 0.01, 128, 2, 64],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 64, 3, 32],\n",
       " [0.010000000000000002, 0.0001, 128, 2, 16],\n",
       " [0.0010000000000000002, 0.01, 16, 2, 64],\n",
       " [0.010000000000000002, 0.0001, 128, 3, 32],\n",
       " [0.010000000000000002, 0.01, 8, 4, 16],\n",
       " [0.1, 0.01, 32, 4, 8],\n",
       " [0.1, 0.0001, 32, 5, 32],\n",
       " [0.010000000000000002, 0.01, 64, 6, 64],\n",
       " [0.1, 1.0000000000000002e-06, 128, 3, 16],\n",
       " [0.1, 0.01, 16, 2, 8],\n",
       " [0.1, 1.0000000000000002e-06, 16, 3, 16],\n",
       " [0.1, 1.0000000000000002e-06, 64, 6, 16],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 16, 3, 32],\n",
       " [0.010000000000000002, 0.01, 16, 5, 64],\n",
       " [0.010000000000000002, 0.0001, 64, 3, 32],\n",
       " [0.0010000000000000002, 0.01, 128, 5, 64],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 16, 4, 16],\n",
       " [0.1, 0.0001, 64, 5, 32],\n",
       " [0.1, 0.0001, 64, 4, 32],\n",
       " [0.1, 0.01, 8, 3, 16],\n",
       " [0.1, 0.01, 32, 4, 16],\n",
       " [0.1, 0.0001, 64, 4, 16],\n",
       " [0.010000000000000002, 0.01, 32, 6, 8],\n",
       " [0.1, 0.01, 32, 6, 32],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 128, 2, 16],\n",
       " [0.1, 0.01, 8, 6, 8],\n",
       " [0.1, 0.0001, 8, 5, 16],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 64, 2, 64],\n",
       " [0.1, 1.0000000000000002e-06, 8, 5, 16],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 16, 2, 64],\n",
       " [0.010000000000000002, 0.01, 8, 5, 32],\n",
       " [0.0010000000000000002, 0.0001, 16, 4, 8],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 64, 4, 32],\n",
       " [0.010000000000000002, 0.0001, 32, 6, 16],\n",
       " [0.010000000000000002, 0.01, 16, 3, 64],\n",
       " [0.1, 1.0000000000000002e-06, 128, 4, 8],\n",
       " [0.1, 0.01, 16, 4, 8],\n",
       " [0.1, 1.0000000000000002e-06, 16, 2, 64],\n",
       " [0.0010000000000000002, 0.01, 8, 3, 16],\n",
       " [0.010000000000000002, 0.01, 64, 6, 64],\n",
       " [0.1, 0.0001, 128, 2, 64],\n",
       " [0.0010000000000000002, 0.01, 32, 3, 64],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 16, 3, 8],\n",
       " [0.1, 1.0000000000000002e-06, 16, 4, 8],\n",
       " [0.1, 1.0000000000000002e-06, 8, 4, 32],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 128, 3, 64],\n",
       " [0.0010000000000000002, 0.0001, 32, 4, 32],\n",
       " [0.010000000000000002, 0.0001, 64, 3, 8],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 128, 5, 32],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 8, 4, 16],\n",
       " [0.1, 0.01, 32, 4, 16],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 8, 3, 32],\n",
       " [0.010000000000000002, 0.01, 16, 5, 32],\n",
       " [0.1, 0.0001, 16, 4, 32],\n",
       " [0.1, 0.0001, 128, 3, 32],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 8, 6, 16],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 8, 6, 8],\n",
       " [0.1, 0.01, 64, 4, 16],\n",
       " [0.010000000000000002, 0.0001, 16, 4, 8],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 64, 6, 32],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 32, 5, 8],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 8, 6, 64],\n",
       " [0.1, 0.0001, 8, 3, 16],\n",
       " [0.1, 0.01, 16, 5, 8],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 64, 3, 16],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 64, 5, 8],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 16, 2, 64],\n",
       " [0.010000000000000002, 0.01, 16, 4, 64],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 128, 5, 64],\n",
       " [0.010000000000000002, 0.0001, 16, 3, 64],\n",
       " [0.010000000000000002, 0.0001, 64, 4, 32],\n",
       " [0.0010000000000000002, 0.0001, 8, 5, 8],\n",
       " [0.1, 0.01, 8, 2, 64],\n",
       " [0.010000000000000002, 0.01, 128, 2, 8],\n",
       " [0.1, 0.01, 64, 2, 8],\n",
       " [0.010000000000000002, 0.0001, 64, 5, 32],\n",
       " [0.1, 1.0000000000000002e-06, 64, 4, 32],\n",
       " [0.0010000000000000002, 0.0001, 16, 6, 32],\n",
       " [0.0010000000000000002, 0.0001, 8, 6, 16],\n",
       " [0.1, 0.01, 128, 4, 64],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 128, 6, 32],\n",
       " [0.0010000000000000002, 0.0001, 16, 2, 16],\n",
       " [0.010000000000000002, 0.0001, 128, 3, 16],\n",
       " [0.010000000000000002, 0.0001, 16, 6, 32],\n",
       " [0.1, 0.0001, 32, 6, 16],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 16, 5, 32],\n",
       " [0.1, 0.0001, 128, 3, 16],\n",
       " [0.010000000000000002, 0.0001, 128, 3, 32],\n",
       " [0.010000000000000002, 0.0001, 16, 5, 32],\n",
       " [0.010000000000000002, 0.01, 32, 3, 8],\n",
       " [0.010000000000000002, 0.0001, 8, 4, 16],\n",
       " [0.1, 1.0000000000000002e-06, 16, 3, 32],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 32, 2, 64],\n",
       " [0.010000000000000002, 0.0001, 128, 4, 16],\n",
       " [0.0010000000000000002, 0.0001, 128, 3, 32],\n",
       " [0.1, 0.0001, 128, 6, 8],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 8, 3, 8],\n",
       " [0.1, 1.0000000000000002e-06, 32, 3, 8],\n",
       " [0.0010000000000000002, 0.01, 8, 4, 32],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 16, 3, 8],\n",
       " [0.1, 0.01, 64, 4, 32],\n",
       " [0.010000000000000002, 0.01, 32, 5, 16],\n",
       " [0.0010000000000000002, 0.01, 128, 2, 32],\n",
       " [0.1, 0.0001, 64, 4, 32],\n",
       " [0.1, 1.0000000000000002e-06, 128, 6, 32],\n",
       " [0.010000000000000002, 1.0000000000000002e-06, 8, 6, 64],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 32, 3, 64],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 16, 4, 64],\n",
       " [0.1, 1.0000000000000002e-06, 64, 4, 8],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 16, 4, 16],\n",
       " [0.1, 0.01, 64, 6, 32],\n",
       " [0.1, 0.01, 8, 5, 16],\n",
       " [0.1, 0.0001, 32, 2, 16],\n",
       " [0.010000000000000002, 0.01, 128, 6, 64],\n",
       " [0.1, 1.0000000000000002e-06, 64, 3, 8],\n",
       " [0.010000000000000002, 0.01, 32, 4, 16],\n",
       " [0.0010000000000000002, 0.0001, 16, 4, 32],\n",
       " [0.1, 0.0001, 16, 3, 8],\n",
       " [0.1, 0.0001, 8, 3, 16],\n",
       " [0.1, 0.0001, 16, 6, 32],\n",
       " [0.1, 0.0001, 8, 6, 64],\n",
       " [0.010000000000000002, 0.01, 32, 3, 8],\n",
       " [0.1, 0.0001, 32, 6, 8],\n",
       " [0.0010000000000000002, 0.0001, 16, 5, 8],\n",
       " [0.0010000000000000002, 0.0001, 8, 6, 32],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 16, 3, 16],\n",
       " [0.1, 0.01, 32, 5, 8],\n",
       " [0.0010000000000000002, 0.01, 32, 6, 16],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 8, 4, 8],\n",
       " [0.0010000000000000002, 0.0001, 128, 4, 64],\n",
       " [0.010000000000000002, 0.01, 8, 2, 64],\n",
       " [0.1, 1.0000000000000002e-06, 32, 3, 8],\n",
       " [0.1, 0.01, 16, 3, 64],\n",
       " [0.1, 0.0001, 8, 3, 32],\n",
       " [0.010000000000000002, 0.0001, 16, 3, 16],\n",
       " [0.010000000000000002, 0.01, 16, 6, 8],\n",
       " [0.010000000000000002, 0.0001, 8, 2, 64],\n",
       " [0.0010000000000000002, 1.0000000000000002e-06, 16, 5, 64],\n",
       " [0.1, 0.0001, 16, 6, 16]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "variants = []\n",
    "\n",
    "NUM_BATCHES = 3\n",
    "NUM_W_D = 3\n",
    "NUM_BATCH_S = 5\n",
    "NUM_S_S = 5\n",
    "NUM_EPS = 4\n",
    "\n",
    "for i in range(NUM_BATCHES * NUM_W_D * NUM_BATCH_S * NUM_S_S):\n",
    "    \n",
    "    lr = 0.1 ** np.random.randint(1,4)\n",
    "    weight_decay = 0.01 ** np.random.randint(1, 4)\n",
    "    batch_size = 2 ** np.random.randint(3,8)\n",
    "    step_size = np.random.randint(2,7)\n",
    "    num_epochs =  2 ** np.random.randint(3,7)\n",
    "    \n",
    "    variants.append([lr, weight_decay, batch_size, step_size, num_epochs])\n",
    "    \n",
    "    \n",
    "variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artem/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:130: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average loss: 9.879964, Train accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's train the model!\n",
    "min_loss_result = 10\n",
    "# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n",
    "for variant in variants:\n",
    "    lr, weight_decay, batch_size, step_size, num_epochs = variant[0], variant[1], variant[2], variant[3], variant[4]\n",
    "\n",
    "    optimizer = optim.Adam(nn_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=0.1)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "    loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, num_epochs)\n",
    "    final_loss = loss_history.pop()\n",
    "    \n",
    "    if final_loss < min_loss_result:\n",
    "        min_loss_result = final_loss\n",
    "        best_model = nn_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(nn_model.parameters(), lr=1e-1, weight_decay=0)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "\n",
    "loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efcd1058fd0>]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training graphs\n",
    "plt.subplot(211)\n",
    "plt.plot(train_history)\n",
    "plt.subplot(212)\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализируем вектора для разного вида слов до и после тренировки\n",
    "\n",
    "В случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных) разделяются семантически.\n",
    "\n",
    "Студенты - в качестве выполненного задания присылайте notebook с диаграммами!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n",
    "assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)\n",
    "\n",
    "def visualize_vectors(input_vectors, output_vectors, title=''):\n",
    "    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n",
    "    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n",
    "\n",
    "    # Helpful words form CS244D example\n",
    "    # http://cs224d.stanford.edu/assignment1/index.html\n",
    "    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n",
    "                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n",
    "                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n",
    "                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n",
    "                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n",
    "                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n",
    "                     }\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.suptitle(title)\n",
    "\n",
    "    for color, words in visualize_words.items():\n",
    "        points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n",
    "        for i, word in enumerate(words):\n",
    "            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n",
    "        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n",
    "\n",
    "visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n",
    "visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
