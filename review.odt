Введение.
В статье Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
https://arxiv.org/pdf/1609.08144.pdf
Рассказывает о новой системе NMT (Neural Machine Translation), которая обладает многими преимуществами по сравнению с предшественниками.
Например, она снизила среднюю ошибку перевода на 60% по сравнению с системами машинного перевода на базе правил (phrase-based systems PBMT) от того же Google.

Данная нейронная сеть на переводческих датасетах Английский-Французский и Английский-Немецкий добилась передовых результатов в данной области.

Немного о датасете, например, о:
WMT’14 English-to-French содержит 36 миллионов пар предложений.
WMT En→De содержит  миллионов пар.

Сетка набирает на датасете WMT’14 English-to-French - 38.95 BLEU.
На WMT’14 English-to-German - 24.17 BLEU.
Где, BLUE — это BLEU (Bilingual Evaluation Understudy) — это измерение различий между автоматическим переводом и одним или несколькими эталонными пользовательскими переводами одного исходного предложения — одна из метрик оценки качества работы системы машинного перевода.

Архитектура модели.
Сетка состоит из трех частей: сеть Энкодер, сеть Attention, и сеть Декодер.
Сеть наследует идеи seq2seq с добавлением attention.

Примечательно, что LSTM, являющая собой RNN encoder реализована не стандартным образом, а с остаточными соединениями (Residual connections).

В энкодере самый первый слой реализован как biderictional lstm, для того чтобы извлекать наибольшее количество контекста. Однако остальные lstm являеются односторонними, из соображений параллелизма системы.

Интересен также подход решения проблемы OOV (out of vocabulary), при котором предложение разбивает на wordpieces, если слово не встречается в словаре, она разбивается на известные для словаря части.


Критерий тренировки.
Также, они сначала учат нормально, а затем, по умолчанию дотачивают модель с помощью Reinforcement learning.


В статье также рассказывают про квантизацию вычислений модели во время ее применения.
Этот прием помогает понизить количество вычислений, необходимое для работы сети.
В рамках метода вычисления происходят не в числах float, а в integer.
Квантизацию инференса не делают в блоке attention и блоке softmax.
Во время обучения квантазацию не используют, а используют только клиппинг аккумуляторов RNN и логитов софтмакса.

А что про декодер?
Примечателен видоизмененный алгоритм поиска луча.

Метрики оценки — конечно, всем известные BLUE, а также применялась оценка людьми -носителями пар языка, которые выставляли оценку от 0 до 6, где 0 — полностью бессмысленный перевод, а 6 — идеальный перевод. Предполагается, что оценка данная человеком лучше отражает качество перевода, чем может дать BLUE.

Сетка тренировалась с помощью Tensorflow.
В качестве оптимизатора использовался гибрид Adam и SGD. Сначала, очень небольшой относительно всей тренировки часть используется Adam с learning rate = 0.0002 , затем применяется SGD с рэйтом 0.5, к которому применяет annealing.

На обучение модели было потрачено 6 дней, используя 96 NVIDIA K80 GPUs.

Во время экспериментов были обучены модели основанный на словах, на символах, смешанный модели, и несколько wordpiece моделей с разными размерами словарей.

Для словесной модели было выбрано 212К частых слов для словаря, а для таргета было выбрано 80К слов для словаря. Неизвестные слова конвертируются в формат, в котором они разделяются от другого текста специальными символами. Механизм внимания копирует эти слова из сурса чтобы не допустить их в декодинг.

Смешанная словесно-символьная модель похожа на словесную модель, однако неизвестные слова делятся на последовательности символов с специальными разграничителями. Словарь таких последовательностей достигал в разных случаях 8К, 16К и 32К.

Наилучший результат на датасете En->Fr показывает Word piece model (38.95 BLEU), при этом обладая достаточным высокой скоростью (0.2118 секунд за декодирование предложения). Модель основанная на символах справляется на удивление хорошо с данным заданием (38.01 BLEU).

Результаты, полученные в работе превосходят остальные бейзлайновские работы, типа LSTM (6 слоев) — (31.5 BLEU), Deep-Att (37.7 BLEU)и проч.
Однако Deep-Att + PosUnk  имеет результат лучше — (39.2 BLEU).

На датасете En-De Wordpiece model собирает также наилучшие результаты.

Ансамбль из 8 моделей дал рекордный (state-of-the-arts) результат.

Ниже приведен пример перевода с английского на французский, показаны исходный текст, перевод данный системой phrase-based, перевод данный GNMT (без RL refinment) и, наконец, человеческий перевод.
Перед каждым переводом дана оценка, данная человеком-оценщиком.Ы

Source — 
"The reason Boeing are doing this is to cram more seats in to make their plane more competitive with our products," said Kevin Keniston, head of passenger comfort at Europe’s Airbus.

PBMT - 3.0
La raison pour laquelle Boeing sont en train de faire, c’est de concentrer davantage de sièges pour prendre leur avion plus compétitive avec nos produits", a déclaré Kevin M. Keniston, chef du confort des passagers de l’Airbus de l’Europe.

GNMT — 6.0
"La raison pour laquelle Boeing fait cela est de créer plus de sièges pour rendre son avion plus compétitif avec nos produits", a déclaré Kevin Keniston, chef du confort des passagers chez Airbus.

Human — 6.0
"Boeing fait ça pour pouvoir caser plus de sièges et rendre ses avions plus compétitifs par rapports à nos produits", a déclaré Kevin Keniston, directeur de Confort Passager chez l’avionneur européen Airbus.

Итог:
1) wordpiece системы значительно увеличивают качество модели
2) распараллеливание вычислений позволяет тренировать такие мощные недели примерно за неделю
3) Квантизация модели сильно ускоряет модель, что делает возможные ее применение в быту.
4) кастомизация beam-seacrh алгоритма также сильно влияет на выход NMT моделей.
